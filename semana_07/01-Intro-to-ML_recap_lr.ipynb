{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "residential-world",
   "metadata": {},
   "source": [
    "# Introducción Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aerial-american",
   "metadata": {},
   "source": [
    "![elgifderigor](https://media.giphy.com/media/NsBknNwmmWE8WU1q2U/giphy.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simple-vault",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Algunos-conceptos\" data-toc-modified-id=\"Algunos-conceptos-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Algunos conceptos</a></span><ul class=\"toc-item\"><li><span><a href=\"#Estimadores\" data-toc-modified-id=\"Estimadores-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Estimadores</a></span></li><li><span><a href=\"#Error-de-Bias-(Sesgo)\" data-toc-modified-id=\"Error-de-Bias-(Sesgo)-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Error de Bias (Sesgo)</a></span></li><li><span><a href=\"#Error-de-Varianza\" data-toc-modified-id=\"Error-de-Varianza-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Error de Varianza</a></span></li></ul></li><li><span><a href=\"#Tipos-de-Aprendizaje-Automático\" data-toc-modified-id=\"Tipos-de-Aprendizaje-Automático-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Tipos de Aprendizaje Automático</a></span></li><li><span><a href=\"#Features\" data-toc-modified-id=\"Features-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Features</a></span></li><li><span><a href=\"#Underfitting-y-Overfitting\" data-toc-modified-id=\"Underfitting-y-Overfitting-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Underfitting y Overfitting</a></span></li><li><span><a href=\"#¿Manos-a-la-obra?\" data-toc-modified-id=\"¿Manos-a-la-obra?-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>¿Manos a la obra?</a></span><ul class=\"toc-item\"><li><span><a href=\"#Practicamos-ML-con-un-ejemplo-de-regresión\" data-toc-modified-id=\"Practicamos-ML-con-un-ejemplo-de-regresión-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Practicamos ML con un ejemplo de regresión</a></span></li><li><span><a href=\"#Cargamos-un-dataset-(aunque-ya-nos-resulta-famliar)\" data-toc-modified-id=\"Cargamos-un-dataset-(aunque-ya-nos-resulta-famliar)-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Cargamos un dataset (aunque ya nos resulta famliar)</a></span></li><li><span><a href=\"#Train-/-Test-Split-¿Esto-qué-es?\" data-toc-modified-id=\"Train-/-Test-Split-¿Esto-qué-es?-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Train / Test Split ¿Esto qué es?</a></span></li><li><span><a href=\"#Preparación-de-los-datos-para-train-test-split\" data-toc-modified-id=\"Preparación-de-los-datos-para-train-test-split-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>Preparación de los datos para train test split</a></span></li><li><span><a href=\"#Parámetros-opcionales-del-train-test-split\" data-toc-modified-id=\"Parámetros-opcionales-del-train-test-split-5.5\"><span class=\"toc-item-num\">5.5&nbsp;&nbsp;</span>Parámetros opcionales del train test split</a></span></li><li><span><a href=\"#Ya-tenemos-el-dataset-dividido,-ahora-vamos-a-importar-un-modelo-y-entrenarlo\" data-toc-modified-id=\"Ya-tenemos-el-dataset-dividido,-ahora-vamos-a-importar-un-modelo-y-entrenarlo-5.6\"><span class=\"toc-item-num\">5.6&nbsp;&nbsp;</span>Ya tenemos el dataset dividido, ahora vamos a importar un modelo y entrenarlo</a></span></li><li><span><a href=\"#Creamos-nuestras-predicciones\" data-toc-modified-id=\"Creamos-nuestras-predicciones-5.7\"><span class=\"toc-item-num\">5.7&nbsp;&nbsp;</span>Creamos nuestras predicciones</a></span></li><li><span><a href=\"#Medimos-nuestro-modelo\" data-toc-modified-id=\"Medimos-nuestro-modelo-5.8\"><span class=\"toc-item-num\">5.8&nbsp;&nbsp;</span>Medimos nuestro modelo</a></span></li></ul></li><li><span><a href=\"#Extra!-Entrenando-diferentes-modelos-a-la-vez\" data-toc-modified-id=\"Extra!-Entrenando-diferentes-modelos-a-la-vez-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Extra! Entrenando diferentes modelos a la vez</a></span><ul class=\"toc-item\"><li><span><a href=\"#Almacenamos-todos-los-modelos-que-vamos-a-entrenar-en-un-diccionario\" data-toc-modified-id=\"Almacenamos-todos-los-modelos-que-vamos-a-entrenar-en-un-diccionario-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Almacenamos todos los modelos que vamos a entrenar en un diccionario</a></span></li><li><span><a href=\"#Iteramos-sobre-los-modelos-para-entrenarlos\" data-toc-modified-id=\"Iteramos-sobre-los-modelos-para-entrenarlos-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Iteramos sobre los modelos para entrenarlos</a></span></li></ul></li><li><span><a href=\"#Selección-de-características\" data-toc-modified-id=\"Selección-de-características-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Selección de características</a></span></li><li><span><a href=\"#Cross-Validation\" data-toc-modified-id=\"Cross-Validation-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Cross-Validation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Hacemos-CV-a-un-solo-modelo\" data-toc-modified-id=\"Hacemos-CV-a-un-solo-modelo-8.1\"><span class=\"toc-item-num\">8.1&nbsp;&nbsp;</span>Hacemos CV a un solo modelo</a></span></li><li><span><a href=\"#CV-iterando-por-todos-los-modelos\" data-toc-modified-id=\"CV-iterando-por-todos-los-modelos-8.2\"><span class=\"toc-item-num\">8.2&nbsp;&nbsp;</span>CV iterando por todos los modelos</a></span></li></ul></li><li><span><a href=\"#Resumen\" data-toc-modified-id=\"Resumen-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Resumen</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25fa2aa",
   "metadata": {},
   "source": [
    "![sklearn](../images/ext_sklearn.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shaped-pharmaceutical",
   "metadata": {},
   "source": [
    "## Algunos conceptos\n",
    "Para entender cómo un algoritmo de aprendizaje automático aprende de los datos para predecir un resultado, es esencial comprender los conceptos subyacentes que intervienen en el entrenamiento de un algoritmo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eastern-aquatic",
   "metadata": {},
   "source": [
    "### Estimadores\n",
    "La estimación es un término estadístico para encontrar alguna estimación de un parámetro desconocido, dados algunos datos. La estimación puntual es el intento de proporcionar la mejor predicción de alguna cantidad de interés.\n",
    "La cantidad de interés puede ser: \n",
    " - Un único parámetro\n",
    " - Un vector de parámetros - por ejemplo, los pesos en la regresión lineal \n",
    " - Una función completa\n",
    "\n",
    "### Error de Bias (Sesgo)\n",
    "\n",
    "Es la diferencia entre la predicción esperada de nuestro modelo y los valores verdaderos. Aunque al final nuestro objetivo es siempre construir modelos que puedan predecir datos muy cercanos a los valores verdaderos, no siempre es tan fácil porque algunos algoritmos son simplemente demasiado rígidos para aprender señales complejas del conjunto de datos.\n",
    "\n",
    "Imagina ajustar una regresión lineal a un conjunto de datos que tiene un patrón no lineal, no importa cuántas observaciones más recopiles, una regresión lineal no podrá modelar las curvas en esos datos. Esto se conoce como ajuste insuficiente.\n",
    "\n",
    "En general, los algoritmos paramétricos como la regresión lineal, tienen un alto bias que los hace rápidos de aprender y más fácil de entender, pero generalmente menos flexibles. A su vez, tienen un menor rendimiento predictivo en problemas complejos.\n",
    "\n",
    "### Error de Varianza\n",
    "\n",
    "Se refiere a la cantidad que la estimación de la función objetivo cambiará si se utiliza diferentes datos de entrenamiento. La función objetivo se estima a partir de los datos de entrenamiento mediante un algoritmo de Machine Learning, por lo que deberíamos esperar que el algoritmo tenga alguna variación. Idealmente no debería cambiar demasiado de un conjunto de datos de entrenamiento a otro, lo que significa que el algoritmo es bueno para elegir el mapeo subyacente oculto entre las variables de entrada y de salida.\n",
    "\n",
    "Los algoritmos de Machine Learning que tienen una gran varianza están fuertemente influenciados por los detalles de los datos de entrenamiento, esto significa que los detalles de la capacitación influyen en el número y los tipos de parámetros utilizados para caracterizar la función de mapeo.\n",
    "\n",
    "Generalmente, los algoritmos de Machine Learning no paramétricos que tienen mucha flexibilidad tienen una gran variación.\n",
    "\n",
    "- Varianza baja: sugiere pequeños cambios en la estimación de la función objetivo con cambios en el conjunto de datos de capacitación.\n",
    "- Alta varianza: sugiere grandes cambios en la estimación de la función objetivo con cambios en el conjunto de datos de capacitación.\n",
    "Los algoritmos de Machine Learning con baja varianza incluyen: regresión lineal, análisis discriminante lineal y regresión logística.\n",
    "\n",
    "Por su parte, los algoritmos con alta varianza son: árboles de decisión, k-vecinos más cercanos y máquinas de vectores de soporte."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ee9c76",
   "metadata": {},
   "source": [
    "<img src=\"https://nvsyashwanth.github.io/machinelearningmaster/assets/images/bias_variance.jpg\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polished-intro",
   "metadata": {},
   "source": [
    "## Tipos de Aprendizaje Automático\n",
    "**- Supervisado**      \n",
    "El modelo se entrena con datos etiquetados, es decir, con datos cuya verdad básica se conoce. Cuando se le presentan datos y una etiqueta, el modelo infiere patrones.      \n",
    "**- No supervisado**          \n",
    "No hay verdades básicas. El modelo busca patrones no detectados previamente, con los que separar los distintos puntos de datos en diferentes clusters.\n",
    "**- Refuerzo**    \n",
    "Tampoco existe una verdad de base. La acción del modelo se valora y se da una recompensa o un castigo en consecuencia. El objetivo del modelo es obtener el mayor número de recompensas posible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blessed-bobby",
   "metadata": {},
   "source": [
    "## Features\n",
    "Las **Features** son variables individuales independientes que actúan como entrada en su sistema. Los modelos de predicción utilizan características para\n",
    "hacer predicciones.      \n",
    "**Objetivo**        \n",
    "El objetivo es la salida de las variables de entrada. Pueden ser las clases individuales a las que se asignan las variables de entrada en el caso de un problema de clasificación o el rango de valores de salida en un problema de regresión."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "racial-nelson",
   "metadata": {},
   "source": [
    "## Underfitting y Overfitting  \n",
    "Dividir un conjunto de datos también puede ser importante para detectar si su modelo sufre uno de los dos problemas más comunes, denominados infraajuste y sobreajuste:\n",
    "\n",
    "El **underfitting** suele ser la consecuencia de que un modelo sea incapaz de encapsular las relaciones entre los datos. Por ejemplo, esto puede ocurrir cuando se intenta representar relaciones no lineales con un modelo lineal. Los modelos infraajustados probablemente tendrán un rendimiento pobre tanto en los conjuntos de entrenamiento como en los de prueba.\n",
    "\n",
    "El **overfitting** suele tener lugar cuando un modelo tiene una estructura excesivamente compleja y aprende tanto las relaciones existentes entre los datos como el ruido. Estos modelos suelen tener una mala capacidad de generalización. Aunque funcionan bien con los datos de entrenamiento, suelen tener un rendimiento pobre con los datos no vistos (de prueba)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42891daf",
   "metadata": {},
   "source": [
    "<img src=\"https://community.alteryx.com/t5/image/serverpage/image-id/52874iE986B6E19F3248CF?v=v2\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3785db62",
   "metadata": {},
   "source": [
    "Sesgo y varianza en el aprendizaje automático --> https://www.analyticslane.com/2019/05/24/los-conceptos-de-sesgo-y-varianza-en-aprendizaje-automaticos/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "varied-apparel",
   "metadata": {},
   "source": [
    "## ¿Manos a la obra?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norwegian-jackson",
   "metadata": {},
   "source": [
    "### Practicamos ML con un ejemplo de regresión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "legitimate-hypothesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_boston"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "explicit-reconstruction",
   "metadata": {},
   "source": [
    "### Cargamos un dataset (aunque ya nos resulta famliar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29aba32f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function load_boston in module sklearn.datasets._base:\n",
      "\n",
      "load_boston(*, return_X_y=False)\n",
      "    DEPRECATED: `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
      "    \n",
      "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
      "    the documentation of this function for further details.\n",
      "    \n",
      "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
      "    dataset unless the purpose of the code is to study and educate about\n",
      "    ethical issues in data science and machine learning.\n",
      "    \n",
      "    In this special case, you can fetch the dataset from the original\n",
      "    source::\n",
      "    \n",
      "        import pandas as pd\n",
      "        import numpy as np\n",
      "    \n",
      "    \n",
      "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
      "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
      "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
      "        target = raw_df.values[1::2, 2]\n",
      "    \n",
      "    Alternative datasets include the California housing dataset (i.e.\n",
      "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
      "    dataset. You can load the datasets as follows::\n",
      "    \n",
      "        from sklearn.datasets import fetch_california_housing\n",
      "        housing = fetch_california_housing()\n",
      "    \n",
      "    for the California housing dataset and::\n",
      "    \n",
      "        from sklearn.datasets import fetch_openml\n",
      "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
      "    \n",
      "    for the Ames housing dataset.\n",
      "    \n",
      "    \n",
      "    Load and return the boston house-prices dataset (regression).\n",
      "    \n",
      "    ==============   ==============\n",
      "    Samples total               506\n",
      "    Dimensionality               13\n",
      "    Features         real, positive\n",
      "    Targets           real 5. - 50.\n",
      "    ==============   ==============\n",
      "    \n",
      "    Read more in the :ref:`User Guide <boston_dataset>`.\n",
      "    \n",
      "    .. deprecated:: 1.0\n",
      "       This function is deprecated in 1.0 and will be removed in 1.2. See the\n",
      "       warning message below for further details regarding the alternative\n",
      "       datasets.\n",
      "    \n",
      "    .. warning::\n",
      "        The Boston housing prices dataset has an ethical problem: as\n",
      "        investigated in [1]_, the authors of this dataset engineered a\n",
      "        non-invertible variable \"B\" assuming that racial self-segregation had a\n",
      "        positive impact on house prices [2]_. Furthermore the goal of the\n",
      "        research that led to the creation of this dataset was to study the\n",
      "        impact of air quality but it did not give adequate demonstration of the\n",
      "        validity of this assumption.\n",
      "    \n",
      "        The scikit-learn maintainers therefore strongly discourage the use of\n",
      "        this dataset unless the purpose of the code is to study and educate\n",
      "        about ethical issues in data science and machine learning.\n",
      "    \n",
      "        In this special case, you can fetch the dataset from the original\n",
      "        source::\n",
      "    \n",
      "            import pandas as pd  # doctest: +SKIP\n",
      "            import numpy as np\n",
      "    \n",
      "    \n",
      "            data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
      "            raw_df = pd.read_csv(data_url, sep=\"s+\", skiprows=22, header=None)\n",
      "            data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
      "            target = raw_df.values[1::2, 2]\n",
      "    \n",
      "        Alternative datasets include the California housing dataset [3]_\n",
      "        (i.e. :func:`~sklearn.datasets.fetch_california_housing`) and Ames\n",
      "        housing dataset [4]_. You can load the datasets as follows::\n",
      "    \n",
      "            from sklearn.datasets import fetch_california_housing\n",
      "            housing = fetch_california_housing()\n",
      "    \n",
      "        for the California housing dataset and::\n",
      "    \n",
      "            from sklearn.datasets import fetch_openml\n",
      "            housing = fetch_openml(name=\"house_prices\", as_frame=True)  # noqa\n",
      "    \n",
      "        for the Ames housing dataset.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    return_X_y : bool, default=False\n",
      "        If True, returns ``(data, target)`` instead of a Bunch object.\n",
      "        See below for more information about the `data` and `target` object.\n",
      "    \n",
      "        .. versionadded:: 0.18\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    data : :class:`~sklearn.utils.Bunch`\n",
      "        Dictionary-like object, with the following attributes.\n",
      "    \n",
      "        data : ndarray of shape (506, 13)\n",
      "            The data matrix.\n",
      "        target : ndarray of shape (506,)\n",
      "            The regression target.\n",
      "        filename : str\n",
      "            The physical location of boston csv dataset.\n",
      "    \n",
      "            .. versionadded:: 0.20\n",
      "    \n",
      "        DESCR : str\n",
      "            The full description of the dataset.\n",
      "        feature_names : ndarray\n",
      "            The names of features\n",
      "    \n",
      "    (data, target) : tuple if ``return_X_y`` is True\n",
      "    \n",
      "        .. versionadded:: 0.18\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "        .. versionchanged:: 0.20\n",
      "            Fixed a wrong data point at [445, 0].\n",
      "    \n",
      "    References\n",
      "    ----------\n",
      "    .. [1] `Racist data destruction? M Carlisle,\n",
      "            <https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8>`_\n",
      "    .. [2] `Harrison Jr, David, and Daniel L. Rubinfeld.\n",
      "           \"Hedonic housing prices and the demand for clean air.\"\n",
      "           Journal of environmental economics and management 5.1 (1978): 81-102.\n",
      "           <https://www.researchgate.net/publication/4974606_Hedonic_housing_prices_and_the_demand_for_clean_air>`_\n",
      "    .. [3] `California housing dataset\n",
      "            <https://scikit-learn.org/stable/datasets/real_world.html#california-housing-dataset>`_\n",
      "    .. [4] `Ames housing dataset\n",
      "            <https://www.openml.org/d/42165>`_\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> import warnings\n",
      "    >>> from sklearn.datasets import load_boston\n",
      "    >>> with warnings.catch_warnings():\n",
      "    ...     # You should probably not use this dataset.\n",
      "    ...     warnings.filterwarnings(\"ignore\")\n",
      "    ...     X, y = load_boston(return_X_y=True)\n",
      "    >>> print(X.shape)\n",
      "    (506, 13)\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "respiratory-elite",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniconda/base/envs/ironhack/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
      "\n",
      "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
      "    the documentation of this function for further details.\n",
      "\n",
      "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
      "    dataset unless the purpose of the code is to study and educate about\n",
      "    ethical issues in data science and machine learning.\n",
      "\n",
      "    In this special case, you can fetch the dataset from the original\n",
      "    source::\n",
      "\n",
      "        import pandas as pd\n",
      "        import numpy as np\n",
      "\n",
      "\n",
      "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
      "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
      "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
      "        target = raw_df.values[1::2, 2]\n",
      "\n",
      "    Alternative datasets include the California housing dataset (i.e.\n",
      "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
      "    dataset. You can load the datasets as follows::\n",
      "\n",
      "        from sklearn.datasets import fetch_california_housing\n",
      "        housing = fetch_california_housing()\n",
      "\n",
      "    for the California housing dataset and::\n",
      "\n",
      "        from sklearn.datasets import fetch_openml\n",
      "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
      "\n",
      "    for the Ames housing dataset.\n",
      "    \n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  MEDV  \n",
       "0     15.3  396.90   4.98  24.0  \n",
       "1     17.8  396.90   9.14  21.6  \n",
       "2     17.8  392.83   4.03  34.7  \n",
       "3     18.7  394.63   2.94  33.4  \n",
       "4     18.7  396.90   5.33  36.2  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "center-bacteria",
   "metadata": {},
   "source": [
    "### Train / Test Split ¿Esto qué es?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75e15f7",
   "metadata": {},
   "source": [
    "<img src=\"https://www.dataquest.io/wp-content/uploads/kaggle_train_test_split.svg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equipped-grove",
   "metadata": {},
   "source": [
    "La división entrenamiento-prueba es una técnica para evaluar el rendimiento de un algoritmo de aprendizaje automático.\n",
    "\n",
    "Puede utilizarse para problemas de clasificación o regresión y puede emplearse para cualquier algoritmo de aprendizaje supervisado.\n",
    "\n",
    "El procedimiento consiste en tomar un conjunto de datos y dividirlo en dos subconjuntos. El primer subconjunto se utiliza para ajustar el modelo y se denomina conjunto de datos de entrenamiento. El segundo subconjunto no se utiliza para entrenar el modelo; en su lugar, se proporciona al modelo el elemento de entrada del conjunto de datos, luego se hacen predicciones y se comparan con los valores esperados. Este segundo conjunto de datos se denomina conjunto de datos de prueba.\n",
    "\n",
    "Conjunto de datos de entrenamiento: Se utiliza para ajustar el modelo de aprendizaje automático.\n",
    "Conjunto de datos de prueba: Se utiliza para evaluar el modelo de aprendizaje automático ajustado.\n",
    "El objetivo es estimar el rendimiento del modelo de aprendizaje automático con datos nuevos: datos no utilizados para entrenar el modelo.\n",
    "\n",
    "Así es como esperamos utilizar el modelo en la práctica. Es decir, ajustarlo sobre los datos disponibles con entradas y salidas conocidas, para luego hacer predicciones sobre nuevos ejemplos en el futuro en los que no tenemos los valores de salida u objetivo esperados.\n",
    "\n",
    "El procedimiento de entrenamiento-prueba es adecuado cuando se dispone de un conjunto de datos suficientemente amplio."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "changing-texas",
   "metadata": {},
   "source": [
    "### Preparación de los datos para train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "automatic-adoption",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX',\n",
       "       'PTRATIO', 'B', 'LSTAT', 'MEDV'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "seasonal-lincoln",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  \n",
       "0     15.3  396.90   4.98  \n",
       "1     17.8  396.90   9.14  \n",
       "2     17.8  392.83   4.03  \n",
       "3     18.7  394.63   2.94  \n",
       "4     18.7  396.90   5.33  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "sealed-realtor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    24.0\n",
       "1    21.6\n",
       "2    34.7\n",
       "3    33.4\n",
       "4    36.2\n",
       "Name: MEDV, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "closed-wellington",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 13)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "intensive-newark",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "friendly-musician",
   "metadata": {},
   "source": [
    "### Parámetros opcionales del train test split\n",
    "\n",
    "**train_size** es el número que define el tamaño del conjunto de entrenamiento. Si proporciona un float, entonces debe estar entre 0.0 y 1.0 y definirá la parte del conjunto de datos utilizado para las pruebas. Si proporciona un int, entonces representará el número total de las muestras de entrenamiento. El valor por defecto es None.\n",
    "\n",
    "**test_size** es el número que define el tamaño del conjunto de pruebas. Es muy similar a train_size. Debe proporcionar o bien train_size o bien test_size. Si no se proporciona ninguno de los dos, la parte por defecto del conjunto de datos que se utilizará para las pruebas es 0,25, o el 25 por ciento.\n",
    "\n",
    "**random_state** es el objeto que controla la aleatoriedad durante la división. Puede ser un int o una instancia de RandomState. El valor por defecto es None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "private-subdivision",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split as tts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4aeb0fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "hybrid-stroke",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "flying-alpha",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404, 13)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "comic-brown",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102, 13)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "destroyed-velvet",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "digital-bradley",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "prerequisite-defense",
   "metadata": {},
   "source": [
    "### Ya tenemos el dataset dividido, ahora vamos a importar un modelo y entrenarlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ultimate-tumor",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression as LinReg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "manual-salmon",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "406734a2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "small-sessions",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4d1f7ade",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.04129075e-01,  4.65481329e-02, -1.23818876e-02,  2.89085987e+00,\n",
       "       -1.99318039e+01,  3.35742356e+00,  5.76915366e-03, -1.62436699e+00,\n",
       "        3.05169695e-01, -1.11631898e-02, -9.85180619e-01,  9.62461378e-03,\n",
       "       -5.52664843e-01])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "96b4b4dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41.57322317711596"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "coastal-migration",
   "metadata": {},
   "source": [
    "### Creamos nuestras predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "executed-composite",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "single-johnson",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([25.15922141, 30.53259902, 18.81930242, 36.11685679, 22.80383238,\n",
       "       23.94786257,  6.91085387, 21.33937806, 18.8919264 , 25.17053759])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0faeb8af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42     25.3\n",
       "236    25.1\n",
       "128    18.0\n",
       "264    36.5\n",
       "64     33.0\n",
       "250    24.4\n",
       "398     5.0\n",
       "339    19.0\n",
       "9      18.9\n",
       "324    25.0\n",
       "Name: MEDV, dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a234859d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2f2c466a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "mediterranean-imaging",
   "metadata": {},
   "source": [
    "### Medimos nuestro modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "treated-remedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ordered-nickel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('MAE - Error Medio Absoluto', 2.9070268228368095)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d2132456",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('MSE - Error Cuadratico Medio', 15.297202577532971)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "87e4490e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('RMSE - Raiz Error Cuadratico Medio', 3.911163839259738)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f8a3624e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('R2 - Coeficiente de Determinacion', 0.7741670650795505)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "frank-banking",
   "metadata": {},
   "source": [
    "## Extra! Entrenando diferentes modelos a la vez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "designed-stomach",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eac9361b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "wrapped-momentum",
   "metadata": {},
   "source": [
    "### Almacenamos todos los modelos que vamos a entrenar en un diccionario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "former-exchange",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ultimate-pierce",
   "metadata": {},
   "source": [
    "### Iteramos sobre los modelos para entrenarlos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "rising-cambridge",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando:  ridge\n",
      "Entrenando:  lasso\n",
      "Entrenando:  sgd\n",
      "Entrenando:  knn\n",
      "Entrenando:  grad\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "bigger-novel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------ridge--------\n",
      "MAE:  2.8287697146165067\n",
      "MSE:  14.949750088767654\n",
      "RMSE:  3.8664906683926774\n",
      "R2:  0.7792965137408717\n",
      "--------lasso--------\n",
      "MAE:  3.2682671634954077\n",
      "MSE:  19.89183386703089\n",
      "RMSE:  4.4600262181999435\n",
      "R2:  0.7063364232530118\n",
      "--------sgd--------\n",
      "MAE:  150943232777295.7\n",
      "MSE:  2.664382981546566e+28\n",
      "RMSE:  163229377917903.2\n",
      "R2:  -3.933434400342483e+26\n",
      "--------knn--------\n",
      "MAE:  4.072549019607844\n",
      "MSE:  30.269105882352946\n",
      "RMSE:  5.501736624226295\n",
      "R2:  0.5531365304092069\n",
      "--------grad--------\n",
      "MAE:  1.8947394591625482\n",
      "MSE:  7.1832212672533\n",
      "RMSE:  2.6801532171227263\n",
      "R2:  0.8939539479362493\n"
     ]
    }
   ],
   "source": [
    "#Podemos seguir el mismo proceso para realizar predicciones de cada algoritmo y sacar métricas\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a04733",
   "metadata": {},
   "source": [
    "## Selección de características"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ecdf71",
   "metadata": {},
   "source": [
    "[Documentación](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b28bcb0",
   "metadata": {},
   "source": [
    "Dado un estimador externo que asigna pesos a las características (por ejemplo, los coeficientes de un modelo lineal), el objetivo de la eliminación recursiva de características (RFE) es seleccionarlas considerando recursivamente conjuntos cada vez más pequeños de características. En primer lugar, el estimador se entrena con el conjunto inicial de características y la importancia de cada una de ellas se obtiene a través de algún atributo específico o llamable. A continuación, se eliminan las características menos importantes del conjunto actual. Este procedimiento se repite recursivamente en el conjunto podado hasta que se alcanza el número deseado de características a seleccionar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "003082cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "40f6d351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102, 13)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f38893d9",
   "metadata": {},
   "source": [
    "La clasificación de las características, de forma que ranking_[i] corresponde a la posición de clasificación de la característica i-ésima. A las características seleccionadas (es decir, las mejores estimadas) se les asigna el rango 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6b1a62df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 5, 3, 1, 1, 1, 6, 2, 7, 8, 1, 9, 1])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "based-mention",
   "metadata": {},
   "source": [
    "## Cross-Validation\n",
    "Leamos un poco la [documentación](https://scikit-learn.org/stable/modules/cross_validation.html)\n",
    "\n",
    "**Wikipedia nos dice**            \n",
    "La validación cruzada o cross-validation es una técnica utilizada para evaluar los resultados de un análisis estadístico y garantizar que son independientes de la partición entre datos de entrenamiento y prueba. Consiste en repetir y calcular la media aritmética obtenida de las medidas de evaluación sobre diferentes particiones. Se utiliza en entornos donde el objetivo principal es la predicción y se quiere estimar la precisión de un modelo que se llevará a cabo a la práctica. Es una técnica muy utilizada en proyectos de inteligencia artificial para validar modelos generados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cellular-orchestra",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Las métricas para el cross validation son el R2 para regresión y el Accuracy para clasificación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disabled-walnut",
   "metadata": {},
   "source": [
    "![kfoldcv](https://scikit-learn.org/stable/_images/grid_search_cross_validation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mobile-affiliation",
   "metadata": {},
   "source": [
    "Cross-Validation: K-fold con 5 splits\n",
    "Lo que hacemos normalmente al entrenar el modelo es pasarle todos los registros y que haga el fit().      \n",
    "Con K-Folds -en este ejemplo de 5 splits- para entrenar, en vez de pasarle todos los registros directamente al modelo, haremos así:\n",
    "\n",
    "**Iterar 5 veces:**      \n",
    "- Apartaremos 1/5 de muestras\n",
    "- Entrenamos al modelo con el restante 4/5 de muestras\n",
    "- Mediremos el r2 obtenido sobre las que habíamos apartado.\n",
    "- Esto quiere decir que hacemos 5 entrenamientos independientes.\n",
    "- El R2 final será el promedio de las 5 R2 anteriores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "central-reservation",
   "metadata": {},
   "source": [
    "### Hacemos CV a un solo modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "joined-payroll",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score as cvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4e0431f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor()"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "beneficial-bulgarian",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "prostate-louis",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.78381036, 0.85404405, 0.74392827, 0.57349182, 0.40462285])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "afraid-advocacy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6719794703856363"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "bd6748c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor()"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "comprehensive-harvey",
   "metadata": {},
   "source": [
    "### CV iterando por todos los modelos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "surprised-information",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo:  ridge Score:  0.3892175824102393\n",
      "Modelo:  lasso Score:  0.431848787926522\n",
      "Modelo:  sgd Score:  -4.790752381957769e+26\n",
      "Modelo:  knn Score:  -0.31501646812514134\n",
      "Modelo:  grad Score:  0.6795550591782186\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "front-being",
   "metadata": {},
   "source": [
    "## Resumen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sunset-opera",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ironhack",
   "language": "python",
   "name": "ironhack"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
